#!/usr/bin/env bash

set -e

exec 200< $0
if ! flock -n 200; then echo "OK: There can be only one query process happening at once. Exit" && exit 1; fi

args=("$@")

# Google Settings
if [[ -z ${args[1]} ]]; then echo "OK: BIGQUERY SQL was not passed. Using container default"; else GOOGLE_GOOGLE_BIGQUERY_SQL=${1}; fi
if [[ -z ${args[2]} ]]; then echo "OK: CLOUDSDK PROJECT was not passed. Using container default"; else GOOGLE_CLOUDSDK_CORE_PROJECT=${2}; fi
if [[ -z ${args[3]} ]]; then echo "OK: BIGQUERY DATASET was not passed. Using container default"; else GOOGLE_BIGQUERY_JOB_DATASET=${3}; fi

# Set the dates
DATE=$(date +"%Y%m%d") && echo "Runnning on $DATE..."
DSTART=${4}
DEND=${5}
START=$(date -d${DSTART} +%s)
END=$(date -d${DEND} +%s)
CUR=${START}

if test -n "${GOOGLE_CLOUDSDK_CRONFILE}"; then
    # Check if the process should run or not.
    if [[ -f "/tmp/runjob.txt" ]]; then echo "OK: Time to run export process..."; else echo "INFO: No export job is present. Exit" && exit 1; fi
else
    echo "OK: Cron is not present"
fi

function setup_gs_storage() {

  # Set the table and file name to the date of the data being extracted
  # We use a "prodcution" and "test" context

  FID=$(cat /dev/urandom | tr -cd [:alnum:] | head -c 8)
  if test "${MODE}" = "prod"; then FILEDATE="${FDATE}"_"${FID}" && GOOGLE_STORAGE_PATH="production"; else FILEDATE="testing_${FDATE}_${FID}" && GOOGLE_STORAGE_PATH="testing"; fi

  # Setup the working bucket to be used for exports
  # Check if the bucket exists
  if ! gsutil ls gs:// | grep -q "gs://${GOOGLE_STORAGE_BUCKET}/"; then
     echo "OK: Creating bucket ${GOOGLE_STORAGE_BUCKET}"
     gsutil mb -p "${GOOGLE_CLOUDSDK_CORE_PROJECT}" -l us gs://"${GOOGLE_STORAGE_BUCKET}"
     gsutil lifecycle set /lifecycle.json gs://"${GOOGLE_STORAGE_BUCKET}"/
  else
     echo "INFO: ${GOOGLE_STORAGE_BUCKET} bucket already exists"
  fi

}

function setup_bq_tables() {

  # Set working dataset and make sure tables auto-expire after an hour
  # We will archive our processed data for audting purposes for 30 days

  GOOGLE_BIGQUERY_WD_DATASET="${GOOGLE_BIGQUERY_JOB_DATASET}_wd"
  GOOGLE_BIGQUERY_ARCHIVE_DATASET="${GOOGLE_BIGQUERY_JOB_DATASET}_archive"

  if ! bq ls -d | grep -q "${GOOGLE_BIGQUERY_JOB_DATASET}_wd"; then
     echo "OK: ${GOOGLE_BIGQUERY_JOB_DATASET}_wd does not exist. Creating..."
     bq mk --default_table_expiration 3600 "${GOOGLE_BIGQUERY_WD_DATASET}"
  else
     echo "INFO: ${GOOGLE_BIGQUERY_JOB_DATASET}_WD dataset already exists"
  fi

  if ! bq ls -d | grep -q "${GOOGLE_BIGQUERY_JOB_DATASET}_archive"; then
     echo "OK: ${GOOGLE_BIGQUERY_JOB_DATASET}_archive does not exist. Creating..."
     bq mk --default_table_expiration 2678400 "${GOOGLE_BIGQUERY_ARCHIVE_DATASET}"
  else
     echo "INFO: ${GOOGLE_BIGQUERY_JOB_DATASET}_archive dataset already exists."
  fi

  #Import the query and set the required BQ variables
  BQQUERY=$(cat /sql/"${GOOGLE_GOOGLE_BIGQUERY_SQL}".sql | sed "s/{{QDATE}}/${QDATE}/g; s/{{GOOGLE_CLOUDSDK_CORE_PROJECT}}/${GOOGLE_CLOUDSDK_CORE_PROJECT}/g; s/{{GOOGLE_BIGQUERY_JOB_DATASET}}/${GOOGLE_BIGQUERY_JOB_DATASET}/g" )

}

function ga_check() {

  # Check if the daily GA360 session data has been loaded by Google
  for i in $(bq ls -n 9999 "${GOOGLE_CLOUDSDK_CORE_PROJECT}":"${GOOGLE_BIGQUERY_JOB_DATASET}" | grep "ga_sessions_${FDATE}" | awk '{print $1}'); do if test "${i}" = "ga_sessions_${FDATE}"; then GASESSIONSCHECK="0"; else GASESSIONSCHECK="1"; fi done

  if test "${GASESSIONSCHECK}" = "0"; then
     echo "OK: ga_sessions_${FDATE} table exists. Run export process..."
     hipchat -i "OK: ${GOOGLE_CLOUDSDK_CORE_PROJECT}:${GOOGLE_BIGQUERY_JOB_DATASET}.ga_sessions_${FDATE} table exists. Run export process..." -l "OK"
     bq query --batch --allow_large_results --destination_table="${GOOGLE_BIGQUERY_WD_DATASET}"."${FILEDATE}"_"${GOOGLE_GOOGLE_BIGQUERY_SQL}" "${BQQUERY}"
     hipchat -i "OK: Completed creating the ${GOOGLE_CLOUDSDK_CORE_PROJECT}:${GOOGLE_BIGQUERY_JOB_DATASET}.ga_sessions_${FDATE} table"
  else
     echo "ERROR: The ga_sessions_${FDATE} data is not present yet. Cant start export process" && hipchat -i "ERROR: The ga_sessions_${FDATE} data is not present yet. Cant start export process" -l "warning" && exit 1
  fi

}

function export_data() {

  # Check if the process created the daily export table in the working job dataset
  for i in $(bq ls -n 9999 "${GOOGLE_CLOUDSDK_CORE_PROJECT}":"${GOOGLE_BIGQUERY_WD_DATASET}" | grep "${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL}" | awk '{print $1}'); do if test "${i}" = "${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL}"; then BQTABLECHECK="0"; else BQTABLECHECK="1"; fi done

  # We will perform a spot check to make sure that the job table in the working dataset does in fact have data present. If it does, run the export process
  if test "${BQTABLECHECK}" = "0"; then

    echo "OK: ${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} table exists. Checking record counts..."
    hipchat -i "OK: ${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} table exists. Checking record counts..." -l "OK"

    while read -r num; do echo "${num}" && if [[ $num =~ \"num\":\"([[:digit:]]+)\" ]] && (( BASH_REMATCH[1] > 1000 )); then echo "Ok: ${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} table count test meet expectations. Ready to creat extracts..."
    hipchat -i "Ok: ${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} table count test meet expectations. Ready to creat extracts..." -l "OK"

    bq extract --compression=GZIP ${GOOGLE_CLOUDSDK_CORE_PROJECT}:${GOOGLE_BIGQUERY_WD_DATASET}.${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} gs://${GOOGLE_STORAGE_BUCKET}/${GOOGLE_STORAGE_PATH}/${GOOGLE_GOOGLE_BIGQUERY_SQL}/${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL}_export*.gz; fi done < <(echo "SELECT COUNT(*) as num FROM [${GOOGLE_CLOUDSDK_CORE_PROJECT}:${GOOGLE_BIGQUERY_WD_DATASET}.${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL}] HAVING COUNT(*) > 100000" | bq query --format json)

    hipchat -i "OK: The GZIP file extracts for ${GOOGLE_CLOUDSDK_CORE_PROJECT}:${GOOGLE_BIGQUERY_WD_DATASET}.${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} have completed" -l "OK"

  else
    echo "ERROR: The ${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} table counts are too low. Exiting" && hipchat -i "ERROR: The ${FILEDATE}_${GOOGLE_GOOGLE_BIGQUERY_SQL} table counts are too low. Exiting" -l "CRITICAL" && exit 1
  fi

}

function transfer_s3() {

  # Transfer to S3
  if [[ "${MODE}" = "prod" && -n ${AWS_S3_BUCKET} ]]; then
    hipchat -i "OK: Begining transfer of files from gs://${GOOGLE_STORAGE_BUCKET}/${GOOGLE_STORAGE_PATH}/${GOOGLE_GOOGLE_BIGQUERY_SQL}/ to s3://${AWS_S3_BUCKET}/ ..." -l "OK"
    gsutil -m rsync -d -r -x "TESTING_" gs://"${GOOGLE_STORAGE_BUCKET}"/"${GOOGLE_STORAGE_PATH}"/"${GOOGLE_GOOGLE_BIGQUERY_SQL}"/ s3://"${AWS_S3_BUCKET}"/
    echo "Ok: Completed S3 transfer"
    hipchat -i "OK: The transfer from gs://${GOOGLE_STORAGE_BUCKET}/${GOOGLE_STORAGE_PATH}/${GOOGLE_GOOGLE_BIGQUERY_SQL}/ to s3://${AWS_S3_BUCKET}/ completed" -l "OK"
  else
    echo "Ok: No S3 Transfer. Running in TEST mode"
  fi

}

function archive_table() {

  # Make an archive of the production table
  if [[ "${MODE}" = "prod" ]]; then
     # Make sure we dont copy test tables
     for i in $(bq ls -n 9999 "${GOOGLE_CLOUDSDK_CORE_PROJECT}" | grep "testing_*" | awk '{print $1}'); do bq rm -ft "${GOOGLE_CLOUDSDK_CORE_PROJECT}"."${i}"; done
     # Transfer to archive dataset
     bq cp "${GOOGLE_CLOUDSDK_CORE_PROJECT}":"${GOOGLE_BIGQUERY_WD_DATASET}"."${FILEDATE}"_"${GOOGLE_GOOGLE_BIGQUERY_SQL}" "${GOOGLE_CLOUDSDK_CORE_PROJECT}":"${GOOGLE_BIGQUERY_ARCHIVE_DATASET}"."${FILEDATE}"_"${GOOGLE_GOOGLE_BIGQUERY_SQL}"_archive
   else
     echo "Ok: No Table Archive. Running in TEST mode"
  fi

}

{
flock -s 200
while [[ ${CUR} -le ${END} ]]; do

  #Set date to proper BQ query format for SQL
  QDATE=$(date -d@${CUR} +%Y-%m-%d)
  FDATE=$(date -d@${CUR} +%Y%m%d)
  let CUR+=24*60*60

  setup_gs_storage
  setup_bq_tables
  ga_check
  export_data
  if [[ -z ${AWS_S3_BUCKET} ]]; then echo "INFO: AWS_S3_BUCKET not set"; else transfer_s3; fi
  archive_table

done
} 200>/tmp/query.lock

# Everything worked. Cleanup and reset the process
echo "OK: Job is complete" && rm -f /tmp/*

hipchat -i "OK: The processing for ${GOOGLE_CLOUDSDK_CORE_PROJECT}:${GOOGLE_BIGQUERY_JOB_DATASET} on ${FDATE} has completed successfully" -l "OK"

exit 0
